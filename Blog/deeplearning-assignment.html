<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5"><title>Cheng Kang (康成)</title><meta name="author" content="Cheng"><link rel="shortcut icon" href="/img/favicon.png"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.13.0/css/all.min.css"><meta name="generator" content="Hexo 7.3.0"></head><body><header id="page_header"><div class="header_wrap"><div id="blog_name"><a class="blog_title" id="site-name" href="/">Cheng Kang (康成)</a></div><button class="menus_icon"><div class="navicon"></div></button><ul class="menus_items"><li class="menus_item"><a class="site-page" href="/#About"> Profile</a></li><li class="menus_item"><a class="site-page" href="/Publications/#index"> Publications</a></li><li class="menus_item"><a class="site-page" href="/Projects/#index"> Projects</a></li><li class="menus_item"><a class="site-page" href="/Volunteer/#index"> Volunteer Experience</a></li><li class="menus_item"><a class="site-page" href="/Blog/#index"> Blog</a></li></ul></div></header><main id="page_main"><div class="side-card sticky"><div class="card-wrap" itemscope itemtype="http://schema.org/Person"><div class="author-avatar"><img class="avatar-img" src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/Kang.jpg'" alt="avatar"></div><div class="author-discrip"><h3>Cheng</h3><p class="author-bio">Cognitive Researcher.</p></div><div class="author-links"><button class="btn m-social-links">Links</button><ul class="social-icons"><li><a class="social-icon" href="https://twitter.com/kangcheng520" target="_blank"><i class="fab fa-twitter" aria-hidden="true"></i></a></li><li><a class="social-icon" href="https://www.facebook.com/kang.cheng.108" target="_blank"><i class="fab fa-facebook-square" aria-hidden="true"></i></a></li><li><a class="social-icon" href="https://github.com/ChengKang520" target="_blank"><i class="fab fa-github" aria-hidden="true"></i></a></li><li><a class="social-icon" href="https://www.linkedin.com/in/kang-cheng-9a7781180/" target="_blank"><i class="fab fa-linkedin" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/img/weixin.png" target="_blank"><i class="fab fa-weixin" aria-hidden="true"></i></a></li><li><a class="social-icon" href="mailto:kangchen@fel.cvut.cz" target="_blank"><i class="fas fa-envelope" aria-hidden="true"></i></a></li></ul><ul class="social-links"><li><a class="e-social-link" href="https://scholar.google.cz/citations?user=2gOnGH4AAAAJ&amp;hl=cs&amp;oi=sra" target="_blank"><i class="fas fa-graduation-cap" aria-hidden="true"></i><span>Google Scholar</span></a></li><li><a class="e-social-link" href="https://orcid.org/0000-0001-9546-4585" target="_blank"><i class="fab fa-orcid" aria-hidden="true"></i><span>ORCID</span></a></li><li><a class="e-social-link" href="/attaches/Kang_CV.pdf" target="_blank"><i class="fas fa-cloud-download-alt" aria-hidden="true"></i><span>CV</span></a></li></ul></div><a class="cv-links" href="/attaches/F3-D-2025-Kang-Cheng-Kang-Cheng-Thesis-2025.pdf" target="_blank"><i class="fas fa-file-pdf" aria-hidden="true"><span>Doctoral Thesis</span></i></a></div></div><div class="page" itemscope itemtype="http://schema.org/CreativeWork"><h2 class="page-title">Deep Learning Basic Knowledge</h2><article><p><span style="font-family:Papyrus; font-size:2em;"> You can find the codes from this: </span><iframe
style="margin-left: 2px; margin-bottom:-5px;"
frameborder="0" scrolling="0" width="100px" height="20px"
src="https://ghbtns.com/github-btn.html?user=chengkang520&repo=deep_learning_AI&type=star&count=true" ></p>
</iframe>


<ol>
<li><a href="#bp_and_overfittint">Backpropagation and Overfitting</a></li>
<li><a href="#pytorch">Using Pytorch and CNN</a></li>
<li><a href="#fine_tuning">Fine-tuning a pretrained CNN</a></li>
<li><a href="#visualization">CNN Visualization: Deep Features, Attention Maps</a></li>
<li><a href="#adversarial_attacks">Adversarial Patterns and Attacks</a></li>
<li><a href="#ve">Gaussian Variational Autoencoders</a></li>
<li><a href="#metric_learning">Metric Learning for COVID-19</a></li>
<li><a href="#chatbot">Chatbot with Pretrained Models</a></li>
<li><a href="#train_transformer">Train Your NLP Models by Transformer</a></li>
<li><a href="#using_AI">Using AI in Applications</a></li>
</ol>
<p>C:\Users\hunter\Documents\MyGithub\hexo-chengkang\source\img\article\DeepLearningImages\Lab1</p>
<hr>
<h2 id="Backpropagation-and-Overfitting"><a href="#Backpropagation-and-Overfitting" class="headerlink" title="Backpropagation and Overfitting "></a>Backpropagation and Overfitting <a name="bp_and_overfittint"></a></h2><p>You can find the code from <a target="_blank" rel="noopener" href="https://github.com/ChengKang520/deep_learning_AI/tree/main/Backpropagation_and_Overfitting">github</a>.</p>
<img style="height:400px" src="/img/article/DeepLearningImages/Lab1/Lab1_figure.jpg"  align=center >

<p>In the above figure, we set $w_{ij}^{(k)}$, and $k \in {(1,2)}$,  $i \in {(1,2,3,4)}$, $j \in {(1,2,3,4})$. The actication functions in the input and the output layers are linear functions: $f_{(0)}(s)&#x3D;s$, $f_{(2)}(s)&#x3D;s$, and the actication functions in the hidden layers is a linear functions: $f_{(0)}(s)&#x3D;0.1s$. Then, the input is $(x_{1}^{(0)},x_{2}^{(0)})&#x3D;(1,1)$, the expected output is $(d_{1},d_{2},d_{3})&#x3D;(1,2,3)$, and the learning rate $\mu$ is $0.1$. </p>
<h3 id="The-output-of-the-neural-network"><a href="#The-output-of-the-neural-network" class="headerlink" title="The output of the neural network"></a>The output of the neural network</h3><h4 id="The-output-of-the-hidden-layer"><a href="#The-output-of-the-hidden-layer" class="headerlink" title="The output of the hidden layer"></a>The output of the hidden layer</h4><p>$x_{1}^{(1)}&#x3D;f_{(1)}(w_{11}^1x_{1}^{(0)}+w_{21}^1x_{2}^{(0)})&#x3D;0.1 \times (1 \times 1+2 \times 1)&#x3D;0.3$</p>
<p>$x_{2}^{(1)}&#x3D;f_{(1)}(w_{12}^1x_{1}^{(0)}+w_{22}^1x_{2}^{(0)})&#x3D;0.1 \times (1 \times 1+2 \times 1)&#x3D;0.3$</p>
<p>$x_{3}^{(1)}&#x3D;f_{(1)}(w_{13}^1x_{1}^{(0)}+w_{23}^1x_{2}^{(0)})&#x3D;0.1 \times (1 \times 1+2 \times 1)&#x3D;0.3$</p>
<p>$x_{4}^{(1)}&#x3D;f_{(1)}(w_{14}^1x_{1}^{(0)}+w_{24}^1x_{2}^{(0)})&#x3D;0.1 \times (1 \times 1+2 \times 1)&#x3D;0.3$</p>
<h4 id="The-output-of-the-output-layer"><a href="#The-output-of-the-output-layer" class="headerlink" title="The output of the output layer"></a>The output of the output layer</h4><p>$x_{1}^{(2)}&#x3D;f_{(2)}(w_{11}^2x_{1}^{(1)}+w_{21}^2x_{2}^{(1)}+w_{31}^2x_{3}^{(1)}+w_{41}^2x_{4}^{(1)})&#x3D;1 \times (1 \times 0.3 \times 4)&#x3D;1.2$</p>
<p>$x_{2}^{(2)}&#x3D;f_{(2)}(w_{12}^2x_{1}^{(1)}+w_{22}^2x_{2}^{(1)}+w_{32}^2x_{3}^{(1)}+w_{42}^2x_{4}^{(1)})&#x3D;1 \times (2 \times 0.3 \times 4)&#x3D;2.4$</p>
<p>$x_{3}^{(2)}&#x3D;f_{(2)}(w_{13}^2x_{1}^{(1)}+w_{23}^2x_{2}^{(1)}+w_{33}^2x_{3}^{(1)}+w_{43}^2x_{4}^{(1)})&#x3D;1 \times (1 \times 0.3 \times 4)&#x3D;1.2$</p>
<h3 id="The-update-of-weights-during-the-backpropagation"><a href="#The-update-of-weights-during-the-backpropagation" class="headerlink" title="The update of weights during the backpropagation"></a>The update of weights during the backpropagation</h3><h4 id="The-error-of-the-output-layer"><a href="#The-error-of-the-output-layer" class="headerlink" title="The error of the output layer"></a>The error of the output layer</h4><p>$\delta_{1}^{(2)}&#x3D;(d_{1}-x_{1}^{(2)})f_{(2)}^{‘}(s_{1}^{(2)})&#x3D;(1-1.2) \times 1&#x3D;-0.2$</p>
<p>$\delta_{2}^{(2)}&#x3D;(d_{2}-x_{2}^{(2)})f_{(2)}^{‘}(s_{2}^{(2)})&#x3D;(2-2.4) \times 1&#x3D;-0.4$</p>
<p>$\delta_{3}^{(2)}&#x3D;(d_{3}-x_{3}^{(2)})f_{(2)}^{‘}(s_{3}^{(2)})&#x3D;(1-1.2) \times 1&#x3D;-0.2$</p>
<h4 id="The-error-of-the-hidden-layer"><a href="#The-error-of-the-hidden-layer" class="headerlink" title="The error of the hidden layer"></a>The error of the hidden layer</h4><p>$\delta_{1}^{(1)}&#x3D;f_{(1)}^{‘}(s_{1}^{(1)}) \sum_{k&#x3D;1}^{3} \delta_{k}^{(2)} w_{1k}^{(2)}&#x3D;0.1 \times (-0.2 \times 1-0.4 \times 2-0.2 \times 1)&#x3D;-0.12$</p>
<p>$\delta_{2}^{(1)}&#x3D;f_{(1)}^{‘}(s_{2}^{(1)}) \sum_{k&#x3D;1}^{3} \delta_{k}^{(2)} w_{2k}^{(2)}&#x3D;0.1 \times (-0.2 \times 1-0.4 \times 2-0.2 \times 1)&#x3D;-0.12$</p>
<p>$\delta_{3}^{(1)}&#x3D;f_{(1)}^{‘}(s_{3}^{(1)}) \sum_{k&#x3D;1}^{3} \delta_{k}^{(2)} w_{3k}^{(2)}&#x3D;0.1 \times (-0.2 \times 1-0.4 \times 2-0.2 \times 1)&#x3D;-0.12$</p>
<p>$\delta_{4}^{(1)}&#x3D;f_{(1)}^{‘}(s_{4}^{(1)}) \sum_{k&#x3D;1}^{3} \delta_{k}^{(2)} w_{4k}^{(2)}&#x3D;0.1 \times (-0.2 \times 1-0.4 \times 2-0.2 \times 1)&#x3D;-0.12$</p>
<h4 id="The-weight-update-between-the-output-layer-and-the-hidden-layer"><a href="#The-weight-update-between-the-output-layer-and-the-hidden-layer" class="headerlink" title="The weight update between the output layer and the hidden layer"></a>The weight update between the output layer and the hidden layer</h4><p>$w_{11}^{(2)}[1]&#x3D;w_{11}^{(2)}[0]+ \mu \delta_{1}^{(2)}x_{1}^{(1)}&#x3D;1+0.1 \times (-0.2) \times 0.3&#x3D;0.994$</p>
<p>then $w_{21}^{(2)}[1]&#x3D;w_{31}^{(2)}[1]&#x3D;w_{41}^{(2)}[1]&#x3D;0.994$.</p>
<p>$w_{12}^{(2)}[1]&#x3D;w_{12}^{(2)}[0]+ \mu \delta_{2}^{(2)}x_{1}^{(1)}&#x3D;2+0.1 \times (-0.4) \times 0.3&#x3D;1.988$</p>
<p>then $w_{22}^{(2)}[1]&#x3D;w_{32}^{(2)}[1]&#x3D;w_{42}^{(2)}[1]&#x3D;1.988$.</p>
<p>$w_{13}^{(2)}[1]&#x3D;w_{13}^{(2)}[0]+ \mu \delta_{3}^{(2)}x_{1}^{(1)}&#x3D;1+0.1 \times (-0.2) \times 0.3&#x3D;0.994$</p>
<p>then $w_{23}^{(2)}[1]&#x3D;w_{33}^{(2)}[1]&#x3D;w_{43}^{(2)}[1]&#x3D;0.994$.</p>
<h4 id="The-weight-update-between-the-input-layer-and-the-hidden-layer"><a href="#The-weight-update-between-the-input-layer-and-the-hidden-layer" class="headerlink" title="The weight update between the input layer and the hidden layer"></a>The weight update between the input layer and the hidden layer</h4><p>$w_{11}^{1}[1]&#x3D;w_{11}^{1}[0]+ \mu \delta_{1}^{(1)}x_{1}^{(0)}&#x3D;1+0.1 \times (-0.12) \times 1&#x3D;0.988$</p>
<p>$w_{12}^{1}[1]&#x3D;w_{12}^{1}[0]+ \mu \delta_{2}^{(1)}x_{1}^{(0)}&#x3D;1+0.1 \times (-0.12) \times 1&#x3D;0.988$</p>
<p>$w_{13}^{1}[1]&#x3D;w_{13}^{1}[0]+ \mu \delta_{3}^{(1)}x_{1}^{(0)}&#x3D;1+0.1 \times (-0.12) \times 1&#x3D;0.988$</p>
<p>$w_{14}^{1}[1]&#x3D;w_{14}^{1}[0]+ \mu \delta_{4}^{(1)}x_{1}^{(0)}&#x3D;1+0.1 \times (-0.12) \times 1&#x3D;0.988$</p>
<p>$w_{21}^{1}[1]&#x3D;w_{21}^{1}[0]+ \mu \delta_{1}^{(1)}x_{2}^{(0)}&#x3D;2+0.1 \times (-0.12) \times 1&#x3D;1.988$</p>
<p>$w_{22}^{1}[1]&#x3D;w_{22}^{1}[0]+ \mu \delta_{2}^{(1)}x_{2}^{(0)}&#x3D;2+0.1 \times (-0.12) \times 1&#x3D;1.988$</p>
<p>$w_{23}^{1}[1]&#x3D;w_{23}^{1}[0]+ \mu \delta_{3}^{(1)}x_{2}^{(0)}&#x3D;2+0.1 \times (-0.12) \times 1&#x3D;1.988$</p>
<p>$w_{24}^{1}[1]&#x3D;w_{24}^{1}[0]+ \mu \delta_{4}^{(1)}x_{2}^{(0)}&#x3D;2+0.1 \times (-0.12) \times 1&#x3D;1.988$</p>
<p>Those above steps are the first round weights updating, and then continue the second, the third….until they satisfy the constrain requirement. </p>
<hr>
<h2 id="Using-Pytorch-and-CNN"><a href="#Using-Pytorch-and-CNN" class="headerlink" title="Using Pytorch and CNN "></a>Using Pytorch and CNN <a name="pytorch"></a></h2><p> You can find the code from <a target="_blank" rel="noopener" href="https://github.com/ChengKang520/deep_learning_AI/tree/main/Using_AI_in_Applications">github</a>.</p>
<h3 id="Getting-Started"><a href="#Getting-Started" class="headerlink" title="Getting Started"></a>Getting Started</h3><p>What is PyTorch: python front end, C++ libraries (A10), Target devices libraries (cuDNN). These will be useful resources for this task:</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://pytorch.org/get-started/locally">Installing PyTorch</a>. Develop and debug locally with CPU&#x2F;GPU on any system. Choose a CUDA version, important to be able to write generic code.</li>
<li><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/index.html">PyTorch docs</a>.</li>
</ul>
<p>Use this script <a target="_blank" rel="noopener" href="https://github.com/ChengKang520/deep_learning_AI/blob/main/Using_Pytorch_and_CNN/mnist.py"><span style="color: blue;">mnist.py</span></a>, and the <span style="font-family:Papyrus; font-size:1.1em;">history.pickle</span>,  the dictionary <span style="font-family:Papyrus; font-size:1.1em;">checkpoint_epoch1.checkpoint</span>, <span style="font-family:Papyrus; font-size:1.1em;">model_cuda.pth</span>, and the <span style="font-family:Papyrus; font-size:1.1em;">optimizer_cuda.pth</span> are saved in the result fold.</p>
<h3 id="Train-and-Test"><a href="#Train-and-Test" class="headerlink" title="Train and Test"></a>Train and Test</h3><p>You can use this script <a target="_blank" rel="noopener" href="https://github.com/ChengKang520/deep_learning_AI/blob/main/Using_Pytorch_and_CNN/plot_loss_train_curve.py"><span style="color: blue;">plot_loss_train_curve.py</span></a> to draw loss and train curves. This figure below shows the accuracy curve and the loss curve. Red dot are the accuracy (loss in the right panel) curve using Test dataset after each one epoch. Black line is the accuracy curve (loss in the right panel) with  exponentially weighted average (EWA), and the partially transparant blue line is the raw accuracy (loss in the right panel) curve. </p>
<table>
<thead>
<tr>
<th align="center">Accuracy Curve</th>
<th align="center">Loss Curve</th>
</tr>
</thead>
<tbody><tr>
<td align="center"><img style="height:300px" src="/img/article/DeepLearningImages/Lab2/lab2_accuracy_plot.png"  align=center ></td>
<td align="center"><img style="height:300px" src="/img/article/DeepLearningImages/Lab2/lab2_loss_plot.png"  align=center ></td>
</tr>
</tbody></table>
<h3 id="tSNE-Confusion-Matrix-and-ROC-Curve"><a href="#tSNE-Confusion-Matrix-and-ROC-Curve" class="headerlink" title="tSNE, Confusion Matrix and ROC Curve"></a>tSNE, Confusion Matrix and ROC Curve</h3><p>You can use this script <a target="_blank" rel="noopener" href="https://github.com/ChengKang520/deep_learning_AI/blob/main/Using_Pytorch_and_CNN/plot_tSNE.py"><span style="color: blue;">plot_tSNE.py</span></a> to tSNE, Confusion Matrix and ROC Curve. For the rSNE, although the boundary is not so perfect, after using nearest neighbor classifier, the accuracy rate can reach 0ver 90%.</p>
<table>
<thead>
<tr>
<th align="center">Example</th>
<th align="center">tSNE</th>
</tr>
</thead>
<tbody><tr>
<td align="center"><img style="height:300px" src="/img/article/DeepLearningImages/Lab2/lab2_example.png"  align=center ></td>
<td align="center"><img style="height:300px" src="/img/article/DeepLearningImages/Lab2/lab2_tSNE_plot.png"  align=center ></td>
</tr>
</tbody></table>
<p>Confusion Matrix<br><img src="/img/article/DeepLearningImages/Lab2/lab2_MNIST_cm1.jpg"  align=center ></p>
<p>ROC Curve<br><img src="/img/article/DeepLearningImages/Lab2/lab2_Multi_class_ROC.jpg"  align=center ></p>
<hr>
<h2 id="Fine-tuning-a-Pretrained-CNN"><a href="#Fine-tuning-a-Pretrained-CNN" class="headerlink" title="Fine-tuning a Pretrained CNN "></a>Fine-tuning a Pretrained CNN <a name="fine_tuning"></a></h2><p>You can find the code from <a target="_blank" rel="noopener" href="https://github.com/ChengKang520/deep_learning_AI/tree/main/Fine_Tuning_a_Pretrained_CNN">github</a>.</p>
<p>You can use one of the following models: <a target="_blank" rel="noopener" href="https://pytorch.org/hub/pytorch_vision_vgg/">VGG11</a> and <a target="_blank" rel="noopener" href="https://pytorch.org/hub/pytorch_vision_squeezenet/">Squeezenet</a>.</p>
<p>Download one of the example datasets for this task:</p>
<ol>
<li><p><a target="_blank" rel="noopener" href="https://cw.fel.cvut.cz/b202/_media/courses/bev033dle/butterflies.zip">Butterflies</a> (35Mb)</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://cw.fel.cvut.cz/b202/_media/courses/bev033dle/naturalist_mix1.zip">iNaturalist mix1</a> (108Mb)</p>
</li>
</ol>
<p>All of the datasets contain color images 224×224 pixels of 10 categories.</p>
<p>For example, different learning rates can effect the accuracy and loss curves .</p>
<table>
<thead>
<tr>
<th align="center">Leaerning Rate</th>
<th align="center">Accuracy Curve</th>
</tr>
</thead>
<tbody><tr>
<td align="center">lr&#x3D;0.0001</td>
<td align="center"><img src="/img/article/DeepLearningImages/Lab3/Lab3_Accuracy_curve_lr_0.0001.jpg"  align=center ></td>
</tr>
<tr>
<td align="center">lr&#x3D;0.01</td>
<td align="center"><img src="/img/article/DeepLearningImages/Lab3/Lab3_Accuracy_curve_lr_0.01.jpg"  align=center ></td>
</tr>
<tr>
<td align="center">lr&#x3D;1</td>
<td align="center"><img src="/img/article/DeepLearningImages/Lab3/Lab3_Accuracy_curve_lr_1.jpg"  align=center ></td>
</tr>
</tbody></table>
<hr>
<h2 id="CNN-Visualization-Deep-Features-Attention-Maps"><a href="#CNN-Visualization-Deep-Features-Attention-Maps" class="headerlink" title="CNN Visualization: Deep Features, Attention Maps "></a>CNN Visualization: Deep Features, Attention Maps <a name="visualization"></a></h2><p> You can find the code from <a target="_blank" rel="noopener" href="https://github.com/ChengKang520/deep_learning_AI/tree/main/CNN_Visualization">github</a>.</p>
<p>For this task we need just one image from ImageNet. We provide an image of a labrador retriever. </p>
<img src="/img/article/DeepLearningImages/Lab4/Lab4_dog.jpg"  align=center >

<p>Besides we need the class codes for the 1000 categories in ImageNet. We provide it as text file <a target="_blank" rel="noopener" href="https://cw.fel.cvut.cz/b202/_media/courses/bev033dle/labs/lab4_visualization/imagenet_classes.txt">imagenet_classes.txt</a></p>
<h3 id="Visualise-the-input-image-and-the-transformed"><a href="#Visualise-the-input-image-and-the-transformed" class="headerlink" title="Visualise the input image and the transformed"></a>Visualise the input image and the transformed</h3><p>Load the image, apply the classifier and report the top 10 classes. Visualise the input image and the transformed (resampled &amp; normalised) tensor image. For the latter you may use the make_grid function mentioned above.</p>
<p>Feature maps.</p>
<img src="/img/article/DeepLearningImages/Lab4/Lab4_graCAM.jpg"  align=center >


<p>Gradien maps.</p>
<img src="/img/article/DeepLearningImages/Lab4/Lab4_gradients.png"  align=center >


<p>Activations of a given layer (activation_max_layer6]).</p>
<img src="/img/article/DeepLearningImages/Lab4/Lab4_activation_max_layer6.png"  align=center >




<hr>
<h2 id="Adversarial-Patterns-and-Attacks"><a href="#Adversarial-Patterns-and-Attacks" class="headerlink" title="Adversarial Patterns and Attacks "></a>Adversarial Patterns and Attacks <a name="adversarial_attacks"></a></h2><p> You can find the code from <a target="_blank" rel="noopener" href="https://github.com/ChengKang520/deep_learning_AI/tree/main/Adversarial_Attacks">github</a>.</p>
<p>In this task, you should choose a clean image which is correctly classified by the net (e.g. the image of the labrador retriever). And then, choose a target class different from the true class (e.g. 892: wall clock) and fix an ε &gt; 0. Implement a projected gradient ascent that aims to maximize the softmax output of the target class w.r.t. the input image, but constrains the search to the ε-ball around the clean image. You can find a text file about the category list: <a target="_blank" rel="noopener" href="https://cw.fel.cvut.cz/b202/_media/courses/bev033dle/labs/lab4_visualization/imagenet_classes.txt">imagenet_classes.txt</a>.<br>The sample is labrador retriever.</p>
<img src="/img/article/DeepLearningImages/Lab5/Lab5_dog.jpg"  align=center >



<h3 id="Fast-Gradient-Sign-Method"><a href="#Fast-Gradient-Sign-Method" class="headerlink" title="Fast Gradient Sign Method"></a>Fast Gradient Sign Method</h3><p>Let’s say we have an input X, which is correctly classified by our model ($M$). We want to find an adversarial example $\widehat{X}$, which is perceptually indistinguishable from original input X, such that it will be misclassified by that same model ($M$). We can do that by adding an adversarial perturbation ($\theta$) to the original input. Note that we want adversarial example to be indistinguishable from the original one. That can be achieved by constraining the magnitude of adversarial perturbation: $|X-\widehat{X} |_{\infty} \leqslant \epsilon$. </p>
<p>That is, the $L_{\infty}$ norm should be less than epsilon. Here, $L_{\infty}$ denotes the maximum changes for all pixels in adversarial example. Fast Gradient Sign Method (FGSM) is a fast and computationally efficient method to generate adversarial examples. However, it usually has a lower success rate. The formula to find adversarial example is as follows:<br>$X^{adv}&#x3D;X+\epsilon \cdot sign(\nabla_{X}J(X,Y_{true}))$</p>
<p>Here,<br>$X$ &#x3D; original (clean) input<br>$X^{adv}$  &#x3D; adversarial input (intentionally designed to be misclassified by our model)<br>$ϵ$  &#x3D; magnitude of adversarial perturbation<br>$\nabla_{X}J(X,Y_{true})$  &#x3D; gradient of loss function w.r.t to input ($X$)</p>
<p>The targeted result is:</p>
<img src="/img/article/DeepLearningImages/Lab5/Lab5_adversarial_attack.png"  align=center >





<hr>
<h2 id="Gaussian-Variational-Autoencoders"><a href="#Gaussian-Variational-Autoencoders" class="headerlink" title="Gaussian Variational Autoencoders "></a>Gaussian Variational Autoencoders <a name="ve"></a></h2><p> You can find the code from <a target="_blank" rel="noopener" href="https://github.com/ChengKang520/deep_learning_AI/tree/main/Gaussian_Variational_Autoencoders">github</a>.</p>
<p>The sample is the MNIST.</p>
<img src="/img/article/DeepLearningImages/Lab6/Lab6_sample_.png"  align=center >



<table>
<thead>
<tr>
<th align="center">Input with Noise</th>
<th align="center">Output with Noise</th>
</tr>
</thead>
<tbody><tr>
<td align="center"><img src="/img/article/DeepLearningImages/Lab6/Lab6_input_with_noise.png"  align=center ></td>
<td align="center"><img src="/img/article/DeepLearningImages/Lab6/Lab6_output_with_noise.png"  align=center ></td>
</tr>
</tbody></table>
<hr>
<h2 id="Metric-Learning-for-COVID-19"><a href="#Metric-Learning-for-COVID-19" class="headerlink" title="Metric Learning for COVID-19 "></a>Metric Learning for COVID-19 <a name="metric_learning"></a></h2><p> You can find the code from <a target="_blank" rel="noopener" href="https://github.com/ChengKang520/deep_learning_AI/tree/main/Metric_Learning">github</a>.</p>
<p> For an anchor (query) image $a$ let the <span style="font-family:Papyrus; font-size:1.1em;">positive</span> example $p$ be of the same class and a <span style="font-family:Papyrus; font-size:1.1em;">negative</span> example $n$ be of different class than $a$. We want that each <span style="font-family:Papyrus; font-size:1.1em;">positive</span> example to be closer to the anchor than all <span style="font-family:Papyrus; font-size:1.1em;">negative</span> examples (a margin $\alpha \geq 0$). The constraint is violated if $d(f_a,f_p)-d(f_a,f_n)+\alpha \geq 0$.<br>Accordingly we define the triplet loss as the total violation of these constraints:<br>$l_a&#x3D;\sum_{p,n}max(d(f_a,f_p)-d(f_a,f_n)+\alpha)$<br>There you can find many <a target="_blank" rel="noopener" href="https://kevinmusgrave.github.io/pytorch-metric-learning/losses/">loss functions</a> for metric learning.</p>
<table>
<thead>
<tr>
<th align="center">Example</th>
<th align="center">tSNE</th>
</tr>
</thead>
<tbody><tr>
<td align="center"><img src="/img/article/DeepLearningImages/Lab7/Lab7_resnet50_Comparring.png"  align=center ></td>
<td align="center"><img src="/img/article/DeepLearningImages/Lab7/Lab7_resnet50_tSNE.png"  align=center ></td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th align="center">Confusion Matrix</th>
<th align="center">ROC Curve</th>
</tr>
</thead>
<tbody><tr>
<td align="center"><img src="/img/article/DeepLearningImages/Lab7/Lab7_Confusion_Matrix.jpg"  align=center ></td>
<td align="center"><img src="/img/article/DeepLearningImages/Lab7/Lab7_Multi-class-ROC.jpg"  align=center ></td>
</tr>
</tbody></table>
<hr>
<h2 id="Chatbot-with-Pretrained-Models"><a href="#Chatbot-with-Pretrained-Models" class="headerlink" title="Chatbot with Pretrained Models "></a>Chatbot with Pretrained Models <a name="chatbot"></a></h2><p> You can find the code from <a target="_blank" rel="noopener" href="https://github.com/ChengKang520/deep_learning_AI/tree/main/Chatbot_with_Pretrained_Models">github</a>.</p>
<p>All of the necessary components to run this project are on the GitHub repository. Feel free to fork the repository and clone it to your local machine. Here’s a quick breakdown of the components:</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/ChengKang520/deep_learning_AI/blob/main/Chatbot_with_Pretrained_Models/train_chatbot.py"><font face="黑体"><strong>train_chatbot.py</strong></font></a> — the code for reading in the natural language data into a training set and using a Keras sequential neural network to create a model</li>
<li><a target="_blank" rel="noopener" href="https://github.com/ChengKang520/deep_learning_AI/blob/main/Chatbot_with_Pretrained_Models/gui_chatbot.py"><font face="黑体"><strong>gui_chatbot.py</strong></font></a> — the code for cleaning up the responses based on the predictions from the model and creating a graphical interface for interacting with the chatbot</li>
<li><a target="_blank" rel="noopener" href="https://github.com/ChengKang520/deep_learning_AI/blob/main/Chatbot_with_Pretrained_Models/classes.pkl"><font face="黑体"><strong>classes.pkl</strong></font></a> — a list of different types of classes of responses</li>
<li><a target="_blank" rel="noopener" href="https://github.com/ChengKang520/deep_learning_AI/blob/main/Chatbot_with_Pretrained_Models/words.pkl"><font face="黑体"><strong>words.pkl</strong></font></a> — a list of different words that could be used for pattern recognition</li>
<li><a target="_blank" rel="noopener" href="https://github.com/ChengKang520/deep_learning_AI/blob/main/Chatbot_with_Pretrained_Models/intents.json"><font face="黑体"><strong>intents.json</strong></font></a> — abunch of JavaScript objects that lists different tags that correspond to different types of word patterns</li>
<li><a target="_blank" rel="noopener" href="https://github.com/ChengKang520/deep_learning_AI/blob/main/Chatbot_with_Pretrained_Models/chatbot_model.h5"><font face="黑体"><strong>chatbot_model.h5</strong></font></a> — the actual model created by <a target="_blank" rel="noopener" href="https://github.com/ChengKang520/deep_learning_AI/blob/main/Chatbot_with_Pretrained_Models/train_chatbot.py"><font face="黑体">train_chatbot.py</font></a> and used by <a target="_blank" rel="noopener" href="https://github.com/ChengKang520/deep_learning_AI/blob/main/Chatbot_with_Pretrained_Models/gui_chatbot.py"><font face="黑体">gui_chatbot.py</font></a>.</li>
</ul>
<p><em><strong>NOTICE</strong></em>: there, if you find that your NLTK cannot import the word_tokenize, and you found that all the packages are installed correctly, See <a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/61041217/why-nltk-word-tokenize-is-not-working-even-after-doing-a-nltk-download-and-all-t">link</a>. And add the command nltk.download(‘punkt’) to your script.</p>
<hr>
<h2 id="Train-Your-NLP-Models-by-Transformer"><a href="#Train-Your-NLP-Models-by-Transformer" class="headerlink" title="Train Your NLP Models by Transformer "></a>Train Your NLP Models by Transformer <a name="train_transformer"></a></h2><p> You can find the code from <a target="_blank" rel="noopener" href="https://github.com/ChengKang520/deep_learning_AI/tree/main/Train_Your_NLP_Models_by_Transformer">github</a>.</p>
<p> Here is the original paper link about T5 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1910.10683">https://arxiv.org/abs/1910.10683</a>. And </p>
<h3 id="T5-fine-tuning"><a href="#T5-fine-tuning" class="headerlink" title="T5 fine-tuning"></a>T5 fine-tuning</h3><p>This notebook is to showcase how to fine-tune T5 model with Huggigface’s Transformers to solve different NLP tasks using text-2-text approach proposed in the T5 paper. For demo I chose 3 non text-2-text problems just to reiterate the fact from the paper that how widely applicable this text-2-text framework is and how it can be used for different tasks without changing the model at all.</p>
<h4 id="Setting-Developing-environment"><a href="#Setting-Developing-environment" class="headerlink" title="Setting Developing environment"></a>Setting Developing environment</h4><p>The virtual developing python environment (python 3.6 + Pycharm).<br>Firstly, installing the virtual tool: </p>
<blockquote>
<p><em>pip3 install virtualenv</em></p>
</blockquote>
<p>And then, </p>
<blockquote>
<p><em>virtualenv TestEnv</em></p>
</blockquote>
<p>or</p>
<blockquote>
<p><em>virtualenv –system-site-packages TestEnv</em></p>
</blockquote>
<p>After that, you should activate the environment.<br>for Windows</p>
<blockquote>
<p><em>source .&#x2F;TestEnv &#x2F;Script&#x2F;activate</em></p>
</blockquote>
<p>for Linux</p>
<blockquote>
<p><em>source .&#x2F;TestEnv &#x2F;bin&#x2F;activate</em></p>
</blockquote>
<p>or quit the virtual environment</p>
<blockquote>
<p><em>deactivate</em></p>
</blockquote>
<h4 id="Preprocessing"><a href="#Preprocessing" class="headerlink" title="Preprocessing"></a>Preprocessing</h4><p>First, install the required packages:</p>
<blockquote>
<p>pip install transformers</p>
</blockquote>
<blockquote>
<p>pip install pytorch_lightning</p>
</blockquote>
<blockquote>
<p>mkdir &#x2F;workspace&#x2F;working&#x2F;t5_tweet</p>
</blockquote>
<blockquote>
<p>mkdir &#x2F;workspace&#x2F;input&#x2F;tweetextract</p>
</blockquote>
<p>We provide the dataset in:<br>&#x2F;workspace&#x2F;input&#x2F;tweetextract&#x2F;val.csv<br>&#x2F;workspace&#x2F;input&#x2F;tweetextract&#x2F;test.csv<br>&#x2F;workspace&#x2F;input&#x2F;tweetextract&#x2F;train.csv</p>
<h4 id="Training-and-Chatting"><a href="#Training-and-Chatting" class="headerlink" title="Training and Chatting"></a>Training and Chatting</h4><p> You can find more detials about T5 mpdel from: <a target="_blank" rel="noopener" href="https://huggingface.co/transformers/model_doc/t5.html">https://huggingface.co/transformers/model_doc/t5.html</a></p>
<p>If you finished the preparatory work, you can excute the training by runing</p>
<blockquote>
<p>python test_t5_gpu.py </p>
</blockquote>
<p>or, if you only have cpus</p>
<blockquote>
<p>python test_t5_cpu.py</p>
</blockquote>
<p>After trainng several epoches, you can save the model in <em><strong>&#x2F;workspace&#x2F;woring&#x2F;t5_tweet&#x2F;</strong></em>. And call the model when using the chatbot by excuting:</p>
<blockquote>
<p>python chatbot_gui.py</p>
</blockquote>
<p>The Chatbot GUI:</p>
<img src="/img/article/DeepLearningImages/Lab9/Lab9_image.png"  align=center >






<hr>
<h2 id="Using-AI-in-Applications"><a href="#Using-AI-in-Applications" class="headerlink" title="Using AI in Applications "></a>Using AI in Applications <a name="using_AI"></a></h2><p>Please read this blog, and you will find a very interesting game.<br><a href="https://chengkang520.github.io/article/tfjs_runing_rabbit/"><span style="font-family:Papyrus; font-size:2em;"> Making an Interactive Running Game with Head or Body Movement Using TensorFlow.js </span></a></p>
<p> You can find the code from <a target="_blank" rel="noopener" href="https://github.com/ChengKang520/running_rabbit_tfjs">github</a>.</p>
<hr>
<table>
<thead>
<tr>
<th>Contributor</th>
<th>Email</th>
</tr>
</thead>
<tbody><tr>
<td>Cheng Kang</td>
<td><a href="mailto:&#x6b;&#x61;&#x6e;&#103;&#x63;&#x68;&#x65;&#x6e;&#64;&#x66;&#x65;&#x6c;&#x2e;&#99;&#118;&#117;&#x74;&#x2e;&#99;&#122;">kangchen@fel.cvut.cz</a></td>
</tr>
<tr>
<td>Xujing Yao</td>
<td><a href="mailto:&#120;&#x79;&#49;&#x34;&#x37;&#64;&#x6c;&#101;&#x69;&#x63;&#101;&#115;&#x74;&#x65;&#114;&#46;&#x61;&#99;&#x2e;&#x75;&#107;">xy147@leicester.ac.uk</a></td>
</tr>
</tbody></table>
</article></div></main><div class="nav-wrap"><div class="nav"><button class="site-nav"><div class="navicon"></div></button><ul class="nav_items"><li class="nav_item"><a class="nav-page" href="/#About"> Profile</a></li><li class="nav_item"><a class="nav-page" href="/Publications/#index"> Publications</a></li><li class="nav_item"><a class="nav-page" href="/Projects/#index"> Projects</a></li><li class="nav_item"><a class="nav-page" href="/Volunteer/#index"> Volunteer Experience</a></li><li class="nav_item"><a class="nav-page" href="/Blog/#index"> Blog</a></li></ul></div><div class="cd-top"><i class="fa fa-arrow-up" aria-hidden="true"></i></div></div><footer id="page_footer"><div class="footer_wrap"><div class="copyright">&copy;2019 - 2025 by Cheng</div><div class="theme-info">Powered by <a target="_blank" href="https://hexo.io" rel="nofollow noopener">Hexo</a> & <a target="_blank" href="https://github.com/PhosphorW/hexo-theme-academia" rel="nofollow noopener">Academia Theme</a></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery-pjax@latest/jquery.pjax.min.js"></script><script src="/js/main.js"></script></body></html>